"""
Orquestrador Prefect para Modern Data Stack
Lakehouse Enterprise - Vers√£o 2.1

RESPONSABILIDADE: Apenas orquestra√ß√£o de componentes
- Airbyte: Ingest√£o de dados (APIs ‚Üí Bronze)
- DBT: Transforma√ß√µes (Bronze ‚Üí Staging ‚Üí Silver ‚Üí Gold)

N√ÉO faz ingest√£o direta de dados!
"""

import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

from prefect import flow, task, get_run_logger
from prefect.blocks.system import Secret
from prefect.task_runners import SequentialTaskRunner
from prefect_airbyte import AirbyteConnection
from prefect_dbt import DbtCoreOperation, DbtCloudJob

import requests
import json


# Configura√ß√µes
AIRBYTE_SERVER_HOST = "airbyte-server.airbyte.svc.cluster.local"
AIRBYTE_SERVER_PORT = 8001
AIRBYTE_API_URL = f"http://{AIRBYTE_SERVER_HOST}:{AIRBYTE_SERVER_PORT}/api/v1"

DBT_PROJECT_DIR = "/opt/prefect/dbt"
DBT_PROFILES_DIR = "/opt/prefect/dbt"

# IDs das conex√µes Airbyte (configurados via UI)
FACEBOOK_ADS_CONNECTION_ID = "facebook-ads-connection"
GOOGLE_ADS_CONNECTION_ID = "google-ads-connection"
ACTIVECAMPAIGN_CONNECTION_ID = "activecampaign-connection"


@task
def check_airbyte_health() -> bool:
    """Verifica se o Airbyte est√° saud√°vel"""
    logger = get_run_logger()
    logger.info("Verificando sa√∫de do Airbyte")
    
    try:
        response = requests.get(f"{AIRBYTE_API_URL}/health", timeout=30)
        response.raise_for_status()
        
        health_data = response.json()
        is_healthy = health_data.get("available", False)
        
        if is_healthy:
            logger.info("Airbyte est√° saud√°vel")
        else:
            logger.warning("Airbyte n√£o est√° saud√°vel")
            
        return is_healthy
        
    except Exception as e:
        logger.error(f"Erro ao verificar sa√∫de do Airbyte: {str(e)}")
        return False


@task
def trigger_airbyte_sync(connection_id: str, connection_name: str) -> Dict[str, Any]:
    """Dispara sincroniza√ß√£o no Airbyte"""
    logger = get_run_logger()
    logger.info(f"Disparando sincroniza√ß√£o Airbyte: {connection_name}")
    
    try:
        # Usar bloco Airbyte do Prefect
        airbyte_connection = AirbyteConnection.load(connection_id)
        
        # Disparar sincroniza√ß√£o
        sync_result = airbyte_connection.trigger_sync()
        
        logger.info(f"Sincroniza√ß√£o {connection_name} iniciada: {sync_result.job_id}")
        
        return {
            "connection_id": connection_id,
            "connection_name": connection_name,
            "job_id": sync_result.job_id,
            "status": "started",
            "started_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Erro ao disparar sincroniza√ß√£o {connection_name}: {str(e)}")
        raise


@task
def wait_for_airbyte_sync(job_info: Dict[str, Any], timeout_minutes: int = 30) -> Dict[str, Any]:
    """Aguarda conclus√£o da sincroniza√ß√£o Airbyte"""
    logger = get_run_logger()
    connection_name = job_info["connection_name"]
    job_id = job_info["job_id"]
    
    logger.info(f"Aguardando conclus√£o da sincroniza√ß√£o {connection_name} (Job: {job_id})")
    
    start_time = time.time()
    timeout_seconds = timeout_minutes * 60
    
    try:
        airbyte_connection = AirbyteConnection.load(job_info["connection_id"])
        
        while True:
            # Verificar status do job
            job_status = airbyte_connection.get_job_status(job_id)
            
            logger.info(f"Status da sincroniza√ß√£o {connection_name}: {job_status}")
            
            if job_status in ["succeeded", "completed"]:
                logger.info(f"Sincroniza√ß√£o {connection_name} conclu√≠da com sucesso")
                return {
                    **job_info,
                    "status": "completed",
                    "completed_at": datetime.now().isoformat()
                }
            elif job_status in ["failed", "cancelled"]:
                logger.error(f"Sincroniza√ß√£o {connection_name} falhou: {job_status}")
                raise Exception(f"Sincroniza√ß√£o {connection_name} falhou com status: {job_status}")
            
            # Verificar timeout
            if time.time() - start_time > timeout_seconds:
                logger.error(f"Timeout na sincroniza√ß√£o {connection_name}")
                raise Exception(f"Timeout aguardando sincroniza√ß√£o {connection_name}")
            
            # Aguardar antes da pr√≥xima verifica√ß√£o
            time.sleep(30)
            
    except Exception as e:
        logger.error(f"Erro aguardando sincroniza√ß√£o {connection_name}: {str(e)}")
        raise


@task
def run_dbt_command(command: str, project_dir: str = DBT_PROJECT_DIR) -> Dict[str, Any]:
    """Executa comando DBT"""
    logger = get_run_logger()
    logger.info(f"Executando comando DBT: {command}")
    
    try:
        dbt_operation = DbtCoreOperation(
            commands=[command],
            project_dir=project_dir,
            profiles_dir=DBT_PROFILES_DIR
        )
        
        result = dbt_operation.run()
        
        logger.info(f"Comando DBT '{command}' executado com sucesso")
        
        return {
            "command": command,
            "status": "success",
            "result": str(result),
            "executed_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Erro executando comando DBT '{command}': {str(e)}")
        raise


@task
def run_dbt_pipeline() -> List[Dict[str, Any]]:
    """Executa pipeline completo do DBT"""
    logger = get_run_logger()
    logger.info("Iniciando pipeline DBT completo")
    
    # Comandos DBT em ordem
    dbt_commands = [
        "dbt deps",           # Instalar depend√™ncias
        "dbt seed",           # Carregar seeds
        "dbt run --models staging",     # Executar modelos staging
        "dbt run --models silver",      # Executar modelos silver (Data Vault)
        "dbt run --models gold",        # Executar modelos gold (Marts)
        "dbt test",           # Executar testes
        "dbt docs generate"   # Gerar documenta√ß√£o
    ]
    
    results = []
    
    for command in dbt_commands:
        try:
            result = run_dbt_command(command)
            results.append(result)
            logger.info(f"‚úÖ {command} - Sucesso")
        except Exception as e:
            logger.error(f"‚ùå {command} - Falhou: {str(e)}")
            # Para comandos cr√≠ticos, interromper pipeline
            if command in ["dbt run --models staging", "dbt run --models silver"]:
                raise
            # Para comandos n√£o cr√≠ticos, continuar
            results.append({
                "command": command,
                "status": "failed",
                "error": str(e),
                "executed_at": datetime.now().isoformat()
            })
    
    logger.info("Pipeline DBT conclu√≠do")
    return results


@task
def send_notification(message: str, status: str = "info"):
    """Envia notifica√ß√£o sobre status do pipeline"""
    logger = get_run_logger()
    logger.info(f"Notifica√ß√£o [{status}]: {message}")
    
    # Aqui voc√™ pode integrar com Slack, Teams, email, etc.
    # Por enquanto, apenas log
    
    notification_data = {
        "message": message,
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "pipeline": "lakehouse-etl"
    }
    
    # Exemplo de integra√ß√£o com webhook (descomente se necess√°rio)
    # webhook_url = os.getenv("NOTIFICATION_WEBHOOK_URL")
    # if webhook_url:
    #     requests.post(webhook_url, json=notification_data)
    
    return notification_data


@flow(
    name="Airbyte Ingestion Orchestrator",
    description="Orquestra ingest√£o de dados via Airbyte"
)
def airbyte_ingestion_flow(connections: List[str] = None) -> List[Dict[str, Any]]:
    """Flow para orquestrar ingest√£o via Airbyte"""
    logger = get_run_logger()
    logger.info("Iniciando orquestra√ß√£o de ingest√£o Airbyte")
    
    # Conex√µes padr√£o se n√£o especificadas
    if connections is None:
        connections = [
            FACEBOOK_ADS_CONNECTION_ID,
            GOOGLE_ADS_CONNECTION_ID,
            ACTIVECAMPAIGN_CONNECTION_ID
        ]
    
    # Verificar sa√∫de do Airbyte
    is_healthy = check_airbyte_health()
    if not is_healthy:
        raise Exception("Airbyte n√£o est√° saud√°vel - abortando pipeline")
    
    # Disparar sincroniza√ß√µes
    sync_jobs = []
    for connection_id in connections:
        try:
            job_info = trigger_airbyte_sync(connection_id, connection_id)
            sync_jobs.append(job_info)
        except Exception as e:
            logger.error(f"Falha ao disparar sincroniza√ß√£o {connection_id}: {str(e)}")
            # Continuar com outras conex√µes
            continue
    
    # Aguardar conclus√£o de todas as sincroniza√ß√µes
    completed_jobs = []
    for job_info in sync_jobs:
        try:
            completed_job = wait_for_airbyte_sync(job_info)
            completed_jobs.append(completed_job)
        except Exception as e:
            logger.error(f"Falha na sincroniza√ß√£o {job_info['connection_name']}: {str(e)}")
            # Continuar com outras sincroniza√ß√µes
            continue
    
    logger.info(f"Ingest√£o Airbyte conclu√≠da: {len(completed_jobs)} sincroniza√ß√µes bem-sucedidas")
    return completed_jobs


@flow(
    name="DBT Transformation Orchestrator", 
    description="Orquestra transforma√ß√µes via DBT"
)
def dbt_transformation_flow() -> List[Dict[str, Any]]:
    """Flow para orquestrar transforma√ß√µes DBT"""
    logger = get_run_logger()
    logger.info("Iniciando orquestra√ß√£o de transforma√ß√µes DBT")
    
    try:
        # Executar pipeline DBT
        dbt_results = run_dbt_pipeline()
        
        # Verificar se houve falhas cr√≠ticas
        failed_critical = [r for r in dbt_results if r["status"] == "failed" and 
                          any(cmd in r["command"] for cmd in ["staging", "silver"])]
        
        if failed_critical:
            raise Exception(f"Falhas cr√≠ticas no DBT: {failed_critical}")
        
        logger.info("Transforma√ß√µes DBT conclu√≠das com sucesso")
        return dbt_results
        
    except Exception as e:
        logger.error(f"Erro nas transforma√ß√µes DBT: {str(e)}")
        raise


@flow(
    name="Daily ETL Orchestrator",
    description="Orquestrador principal do pipeline ETL di√°rio",
    task_runner=SequentialTaskRunner()
)
def daily_etl_orchestrator(
    run_ingestion: bool = True,
    run_transformations: bool = True,
    connections: List[str] = None
):
    """Flow principal que orquestra todo o pipeline ETL"""
    logger = get_run_logger()
    logger.info("üöÄ Iniciando pipeline ETL di√°rio")
    
    pipeline_start = datetime.now()
    
    try:
        # Notifica√ß√£o de in√≠cio
        send_notification("Pipeline ETL di√°rio iniciado", "info")
        
        ingestion_results = []
        transformation_results = []
        
        # Fase 1: Ingest√£o via Airbyte
        if run_ingestion:
            logger.info("üì• Fase 1: Ingest√£o de dados via Airbyte")
            ingestion_results = airbyte_ingestion_flow(connections)
            
            if not ingestion_results:
                logger.warning("Nenhuma ingest√£o bem-sucedida - continuando com transforma√ß√µes")
            else:
                logger.info(f"‚úÖ Ingest√£o conclu√≠da: {len(ingestion_results)} fontes")
        
        # Fase 2: Transforma√ß√µes via DBT
        if run_transformations:
            logger.info("üîÑ Fase 2: Transforma√ß√µes via DBT")
            transformation_results = dbt_transformation_flow()
            logger.info("‚úÖ Transforma√ß√µes conclu√≠das")
        
        # Pipeline conclu√≠do com sucesso
        pipeline_end = datetime.now()
        duration = pipeline_end - pipeline_start
        
        success_message = f"""
        ‚úÖ Pipeline ETL conclu√≠do com sucesso!
        
        üìä Resumo:
        - Dura√ß√£o: {duration}
        - Ingest√µes: {len(ingestion_results)}
        - Transforma√ß√µes DBT: {len(transformation_results)}
        - In√≠cio: {pipeline_start.strftime('%Y-%m-%d %H:%M:%S')}
        - Fim: {pipeline_end.strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        send_notification(success_message, "success")
        logger.info(success_message)
        
        return {
            "status": "success",
            "duration": str(duration),
            "ingestion_results": ingestion_results,
            "transformation_results": transformation_results,
            "started_at": pipeline_start.isoformat(),
            "completed_at": pipeline_end.isoformat()
        }
        
    except Exception as e:
        # Pipeline falhou
        pipeline_end = datetime.now()
        duration = pipeline_end - pipeline_start
        
        error_message = f"""
        ‚ùå Pipeline ETL falhou!
        
        üö® Erro: {str(e)}
        - Dura√ß√£o at√© falha: {duration}
        - In√≠cio: {pipeline_start.strftime('%Y-%m-%d %H:%M:%S')}
        - Falha: {pipeline_end.strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        send_notification(error_message, "error")
        logger.error(error_message)
        
        raise


@flow(
    name="Hourly Incremental Sync",
    description="Sincroniza√ß√£o incremental a cada hora"
)
def hourly_incremental_sync():
    """Flow para sincroniza√ß√£o incremental hor√°ria"""
    logger = get_run_logger()
    logger.info("üîÑ Iniciando sincroniza√ß√£o incremental hor√°ria")
    
    # Apenas ingest√£o incremental, sem transforma√ß√µes completas
    ingestion_results = airbyte_ingestion_flow()
    
    # Executar apenas modelos incrementais do DBT
    incremental_commands = [
        "dbt run --models staging --select state:modified+",
        "dbt run --models silver --select state:modified+",
        "dbt run --models gold --select state:modified+"
    ]
    
    for command in incremental_commands:
        try:
            run_dbt_command(command)
        except Exception as e:
            logger.warning(f"Comando incremental falhou: {command} - {str(e)}")
            # Continuar com pr√≥ximo comando
            continue
    
    logger.info("‚úÖ Sincroniza√ß√£o incremental conclu√≠da")


if __name__ == "__main__":
    # Para execu√ß√£o local/teste
    daily_etl_orchestrator()

